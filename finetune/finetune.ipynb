{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d118f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103de286",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.IMAGENET1K_V1\n",
    "model = resnet50(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c234c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e5b5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_indices = {\n",
    "    # Local Index : ImageNet Index \n",
    "    0: 418,  # ballpoint\n",
    "    1: 436,  # car\n",
    "    2: 504,  # coffeemug\n",
    "    3: 504,  # cup (using the same index as coffeemug, common for similar items)\n",
    "    4: 546,  # electricguitar\n",
    "    5: 620,  # laptop\n",
    "    6: 651,  # microwave\n",
    "    7: 673,  # mouse\n",
    "    8: 681, # notebook\n",
    "    9: 837, # sunglasses\n",
    "    10: 859, # toaster\n",
    "    11: 898, # waterbottle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77034ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_to_imagenet_map: Dict[int, int] = {i: imagenet_indices[i] for i in range(len(imagenet_indices))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "501e074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'finetune_dataset' \n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001 # Critical: Use a low LR for full finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c40f312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFolder(ImageFolder):\n",
    "    \"\"\"\n",
    "    Overrides the ImageFolder class to map local class indices (0-13) \n",
    "    to their corresponding global ImageNet indices (0-999).\n",
    "    \"\"\"\n",
    "    def __init__(self, root: str, transform=None, index_map: Dict[int, int] = None):\n",
    "        super().__init__(root, transform)\n",
    "        self.index_map = index_map\n",
    "\n",
    "        if self.index_map is None:\n",
    "            raise ValueError(\"index_map must be provided for ImageNet finetuning.\")\n",
    "            \n",
    "        # Re-map the self.samples list to use the global index as the target\n",
    "        new_samples = []\n",
    "        for path, local_idx in self.samples:\n",
    "            global_idx = self.index_map.get(local_idx)\n",
    "            if global_idx is not None:\n",
    "                new_samples.append((path, global_idx))\n",
    "            else:\n",
    "                print(f\"Warning: Missing ImageNet index for local class {local_idx}\")\n",
    "                \n",
    "        self.samples = new_samples\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Modified to ensure the returned target is the ImageNet index.\n",
    "        \"\"\"\n",
    "        path, target = self.samples[index] # target is now the global ImageNet index\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b62b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(weights.transforms().mean, weights.transforms().std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(weights.transforms().mean, weights.transforms().std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83ace040",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    image_datasets = {\n",
    "        x: CustomImageFolder(os.path.join(data_dir, x), \n",
    "                             data_transforms[x], \n",
    "                             index_map=local_to_imagenet_map) \n",
    "        for x in ['train', 'val']\n",
    "    }\n",
    "    dataloaders = {\n",
    "        x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=0) \n",
    "        for x in ['train', 'val']\n",
    "    }\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nFATAL ERROR: Could not find data directory or split: {e}\")\n",
    "    print(f\"Please ensure your data structure looks like this: {data_dir}/train/ballpoint/... and {data_dir}/val/ballpoint/...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d55947d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Finetuning ALL layers: apply optimizer to all model parameters\n",
    "optimizer_ft = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Decays the learning rate by a factor of 0.1 every 7 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70e4a97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \"\"\"General function to train and validate a model.\"\"\"\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    print(\"--- Starting Full Finetuning ---\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 25)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device) # Labels are now ImageNet indices\n",
    "\n",
    "                optimizer.zero_grad() \n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Deep copy the model if it has the best validation accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d65e9af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Finetuning ---\n",
      "\n",
      "Epoch 0/99\n",
      "-------------------------\n",
      "train Loss: 4.8042 Acc: 0.1915\n",
      "val Loss: 2.6095 Acc: 0.5306\n",
      "\n",
      "Epoch 1/99\n",
      "-------------------------\n",
      "train Loss: 4.2769 Acc: 0.2979\n",
      "val Loss: 2.5687 Acc: 0.5510\n",
      "\n",
      "Epoch 2/99\n",
      "-------------------------\n",
      "train Loss: 4.4815 Acc: 0.2766\n",
      "val Loss: 2.4975 Acc: 0.5510\n",
      "\n",
      "Epoch 3/99\n",
      "-------------------------\n",
      "train Loss: 4.0607 Acc: 0.3191\n",
      "val Loss: 2.3798 Acc: 0.5714\n",
      "\n",
      "Epoch 4/99\n",
      "-------------------------\n",
      "train Loss: 3.8601 Acc: 0.3617\n",
      "val Loss: 2.2325 Acc: 0.5714\n",
      "\n",
      "Epoch 5/99\n",
      "-------------------------\n",
      "train Loss: 3.5140 Acc: 0.4681\n",
      "val Loss: 2.0454 Acc: 0.6122\n",
      "\n",
      "Epoch 6/99\n",
      "-------------------------\n",
      "train Loss: 3.5766 Acc: 0.4468\n",
      "val Loss: 1.8542 Acc: 0.6327\n",
      "\n",
      "Epoch 7/99\n",
      "-------------------------\n",
      "train Loss: 3.0474 Acc: 0.4894\n",
      "val Loss: 1.6940 Acc: 0.6327\n",
      "\n",
      "Epoch 8/99\n",
      "-------------------------\n",
      "train Loss: 3.0853 Acc: 0.5106\n",
      "val Loss: 1.5503 Acc: 0.6939\n",
      "\n",
      "Epoch 9/99\n",
      "-------------------------\n",
      "train Loss: 2.4884 Acc: 0.5106\n",
      "val Loss: 1.4233 Acc: 0.7143\n",
      "\n",
      "Epoch 10/99\n",
      "-------------------------\n",
      "train Loss: 2.6714 Acc: 0.4894\n",
      "val Loss: 1.2848 Acc: 0.7347\n",
      "\n",
      "Epoch 11/99\n",
      "-------------------------\n",
      "train Loss: 2.2017 Acc: 0.5532\n",
      "val Loss: 1.1958 Acc: 0.7959\n",
      "\n",
      "Epoch 12/99\n",
      "-------------------------\n",
      "train Loss: 2.1322 Acc: 0.6809\n",
      "val Loss: 1.0465 Acc: 0.8163\n",
      "\n",
      "Epoch 13/99\n",
      "-------------------------\n",
      "train Loss: 2.0854 Acc: 0.6596\n",
      "val Loss: 0.9466 Acc: 0.8163\n",
      "\n",
      "Epoch 14/99\n",
      "-------------------------\n",
      "train Loss: 1.7619 Acc: 0.6809\n",
      "val Loss: 0.8338 Acc: 0.8571\n",
      "\n",
      "Epoch 15/99\n",
      "-------------------------\n",
      "train Loss: 1.9686 Acc: 0.6596\n",
      "val Loss: 0.8406 Acc: 0.8163\n",
      "\n",
      "Epoch 16/99\n",
      "-------------------------\n",
      "train Loss: 1.6392 Acc: 0.7234\n",
      "val Loss: 0.8367 Acc: 0.8163\n",
      "\n",
      "Epoch 17/99\n",
      "-------------------------\n",
      "train Loss: 1.8566 Acc: 0.6809\n",
      "val Loss: 0.7870 Acc: 0.8163\n",
      "\n",
      "Epoch 18/99\n",
      "-------------------------\n",
      "train Loss: 1.7179 Acc: 0.6596\n",
      "val Loss: 0.8061 Acc: 0.8367\n",
      "\n",
      "Epoch 19/99\n",
      "-------------------------\n",
      "train Loss: 1.6470 Acc: 0.7447\n",
      "val Loss: 0.7757 Acc: 0.8571\n",
      "\n",
      "Epoch 20/99\n",
      "-------------------------\n",
      "train Loss: 1.5419 Acc: 0.6809\n",
      "val Loss: 0.7661 Acc: 0.8367\n",
      "\n",
      "Epoch 21/99\n",
      "-------------------------\n",
      "train Loss: 1.7785 Acc: 0.7021\n",
      "val Loss: 0.7682 Acc: 0.8776\n",
      "\n",
      "Epoch 22/99\n",
      "-------------------------\n",
      "train Loss: 1.5246 Acc: 0.7872\n",
      "val Loss: 0.7576 Acc: 0.8776\n",
      "\n",
      "Epoch 23/99\n",
      "-------------------------\n",
      "train Loss: 1.6366 Acc: 0.7447\n",
      "val Loss: 0.7545 Acc: 0.8980\n",
      "\n",
      "Epoch 24/99\n",
      "-------------------------\n",
      "train Loss: 1.3561 Acc: 0.7660\n",
      "val Loss: 0.7587 Acc: 0.8163\n",
      "\n",
      "Epoch 25/99\n",
      "-------------------------\n",
      "train Loss: 1.6259 Acc: 0.7021\n",
      "val Loss: 0.7629 Acc: 0.8367\n",
      "\n",
      "Epoch 26/99\n",
      "-------------------------\n",
      "train Loss: 1.7133 Acc: 0.6809\n",
      "val Loss: 0.7853 Acc: 0.8367\n",
      "\n",
      "Epoch 27/99\n",
      "-------------------------\n",
      "train Loss: 1.3718 Acc: 0.7447\n",
      "val Loss: 0.7880 Acc: 0.8163\n",
      "\n",
      "Epoch 28/99\n",
      "-------------------------\n",
      "train Loss: 1.7791 Acc: 0.6809\n",
      "val Loss: 0.7603 Acc: 0.8367\n",
      "\n",
      "Epoch 29/99\n",
      "-------------------------\n",
      "train Loss: 1.5236 Acc: 0.7234\n",
      "val Loss: 0.7792 Acc: 0.8367\n",
      "\n",
      "Epoch 30/99\n",
      "-------------------------\n",
      "train Loss: 1.3855 Acc: 0.7872\n",
      "val Loss: 0.7779 Acc: 0.8163\n",
      "\n",
      "Epoch 31/99\n",
      "-------------------------\n",
      "train Loss: 1.7700 Acc: 0.6596\n",
      "val Loss: 0.7553 Acc: 0.8367\n",
      "\n",
      "Epoch 32/99\n",
      "-------------------------\n",
      "train Loss: 1.5889 Acc: 0.7021\n",
      "val Loss: 0.7627 Acc: 0.8367\n",
      "\n",
      "Epoch 33/99\n",
      "-------------------------\n",
      "train Loss: 1.7890 Acc: 0.7021\n",
      "val Loss: 0.7230 Acc: 0.8571\n",
      "\n",
      "Epoch 34/99\n",
      "-------------------------\n",
      "train Loss: 1.6855 Acc: 0.7021\n",
      "val Loss: 0.7302 Acc: 0.8367\n",
      "\n",
      "Epoch 35/99\n",
      "-------------------------\n",
      "train Loss: 1.7113 Acc: 0.7660\n",
      "val Loss: 0.7267 Acc: 0.8571\n",
      "\n",
      "Epoch 36/99\n",
      "-------------------------\n",
      "train Loss: 1.5669 Acc: 0.7660\n",
      "val Loss: 0.7180 Acc: 0.8163\n",
      "\n",
      "Epoch 37/99\n",
      "-------------------------\n",
      "train Loss: 1.2628 Acc: 0.7447\n",
      "val Loss: 0.7277 Acc: 0.8163\n",
      "\n",
      "Epoch 38/99\n",
      "-------------------------\n",
      "train Loss: 1.7792 Acc: 0.6809\n",
      "val Loss: 0.7196 Acc: 0.8367\n",
      "\n",
      "Epoch 39/99\n",
      "-------------------------\n",
      "train Loss: 1.5047 Acc: 0.7234\n",
      "val Loss: 0.7093 Acc: 0.8367\n",
      "\n",
      "Epoch 40/99\n",
      "-------------------------\n",
      "train Loss: 1.2597 Acc: 0.8298\n",
      "val Loss: 0.7129 Acc: 0.8367\n",
      "\n",
      "Epoch 41/99\n",
      "-------------------------\n",
      "train Loss: 1.2373 Acc: 0.7872\n",
      "val Loss: 0.7170 Acc: 0.8367\n",
      "\n",
      "Epoch 42/99\n",
      "-------------------------\n",
      "train Loss: 1.3799 Acc: 0.7447\n",
      "val Loss: 0.7250 Acc: 0.8571\n",
      "\n",
      "Epoch 43/99\n",
      "-------------------------\n",
      "train Loss: 1.2824 Acc: 0.8085\n",
      "val Loss: 0.7183 Acc: 0.8367\n",
      "\n",
      "Epoch 44/99\n",
      "-------------------------\n",
      "train Loss: 1.5542 Acc: 0.6809\n",
      "val Loss: 0.7005 Acc: 0.8571\n",
      "\n",
      "Epoch 45/99\n",
      "-------------------------\n",
      "train Loss: 1.5890 Acc: 0.6809\n",
      "val Loss: 0.7072 Acc: 0.8571\n",
      "\n",
      "Epoch 46/99\n",
      "-------------------------\n",
      "train Loss: 1.3695 Acc: 0.7447\n",
      "val Loss: 0.6821 Acc: 0.8776\n",
      "\n",
      "Epoch 47/99\n",
      "-------------------------\n",
      "train Loss: 1.3004 Acc: 0.7660\n",
      "val Loss: 0.6938 Acc: 0.8776\n",
      "\n",
      "Epoch 48/99\n",
      "-------------------------\n",
      "train Loss: 1.2000 Acc: 0.7447\n",
      "val Loss: 0.7005 Acc: 0.8776\n",
      "\n",
      "Epoch 49/99\n",
      "-------------------------\n",
      "train Loss: 1.5020 Acc: 0.7021\n",
      "val Loss: 0.6918 Acc: 0.8776\n",
      "\n",
      "Epoch 50/99\n",
      "-------------------------\n",
      "train Loss: 1.7090 Acc: 0.7234\n",
      "val Loss: 0.6994 Acc: 0.8776\n",
      "\n",
      "Epoch 51/99\n",
      "-------------------------\n",
      "train Loss: 1.4942 Acc: 0.8085\n",
      "val Loss: 0.7024 Acc: 0.8776\n",
      "\n",
      "Epoch 52/99\n",
      "-------------------------\n",
      "train Loss: 1.4024 Acc: 0.8085\n",
      "val Loss: 0.7231 Acc: 0.8571\n",
      "\n",
      "Epoch 53/99\n",
      "-------------------------\n",
      "train Loss: 1.4295 Acc: 0.7447\n",
      "val Loss: 0.7079 Acc: 0.8367\n",
      "\n",
      "Epoch 54/99\n",
      "-------------------------\n",
      "train Loss: 1.3657 Acc: 0.7872\n",
      "val Loss: 0.7020 Acc: 0.8571\n",
      "\n",
      "Epoch 55/99\n",
      "-------------------------\n",
      "train Loss: 1.2427 Acc: 0.7660\n",
      "val Loss: 0.7198 Acc: 0.8571\n",
      "\n",
      "Epoch 56/99\n",
      "-------------------------\n",
      "train Loss: 1.4439 Acc: 0.8085\n",
      "val Loss: 0.7129 Acc: 0.8571\n",
      "\n",
      "Epoch 57/99\n",
      "-------------------------\n",
      "train Loss: 1.4537 Acc: 0.7660\n",
      "val Loss: 0.7008 Acc: 0.8980\n",
      "\n",
      "Epoch 58/99\n",
      "-------------------------\n",
      "train Loss: 1.5183 Acc: 0.7021\n",
      "val Loss: 0.6985 Acc: 0.8776\n",
      "\n",
      "Epoch 59/99\n",
      "-------------------------\n",
      "train Loss: 0.9567 Acc: 0.8723\n",
      "val Loss: 0.7035 Acc: 0.8776\n",
      "\n",
      "Epoch 60/99\n",
      "-------------------------\n",
      "train Loss: 1.4603 Acc: 0.7447\n",
      "val Loss: 0.6971 Acc: 0.8571\n",
      "\n",
      "Epoch 61/99\n",
      "-------------------------\n",
      "train Loss: 1.3321 Acc: 0.7872\n",
      "val Loss: 0.6955 Acc: 0.8571\n",
      "\n",
      "Epoch 62/99\n",
      "-------------------------\n",
      "train Loss: 1.3800 Acc: 0.7447\n",
      "val Loss: 0.7073 Acc: 0.8571\n",
      "\n",
      "Epoch 63/99\n",
      "-------------------------\n",
      "train Loss: 1.6698 Acc: 0.6809\n",
      "val Loss: 0.6992 Acc: 0.8367\n",
      "\n",
      "Epoch 64/99\n",
      "-------------------------\n",
      "train Loss: 1.3085 Acc: 0.8085\n",
      "val Loss: 0.6946 Acc: 0.8367\n",
      "\n",
      "Epoch 65/99\n",
      "-------------------------\n",
      "train Loss: 1.2642 Acc: 0.7447\n",
      "val Loss: 0.6804 Acc: 0.8571\n",
      "\n",
      "Epoch 66/99\n",
      "-------------------------\n",
      "train Loss: 1.2587 Acc: 0.7872\n",
      "val Loss: 0.6991 Acc: 0.8367\n",
      "\n",
      "Epoch 67/99\n",
      "-------------------------\n",
      "train Loss: 1.4128 Acc: 0.7447\n",
      "val Loss: 0.7080 Acc: 0.8367\n",
      "\n",
      "Epoch 68/99\n",
      "-------------------------\n",
      "train Loss: 1.5603 Acc: 0.6596\n",
      "val Loss: 0.7353 Acc: 0.8367\n",
      "\n",
      "Epoch 69/99\n",
      "-------------------------\n",
      "train Loss: 1.7036 Acc: 0.7660\n",
      "val Loss: 0.7169 Acc: 0.8367\n",
      "\n",
      "Epoch 70/99\n",
      "-------------------------\n",
      "train Loss: 1.4221 Acc: 0.7872\n",
      "val Loss: 0.7144 Acc: 0.8571\n",
      "\n",
      "Epoch 71/99\n",
      "-------------------------\n",
      "train Loss: 1.4072 Acc: 0.7872\n",
      "val Loss: 0.7056 Acc: 0.8367\n",
      "\n",
      "Epoch 72/99\n",
      "-------------------------\n",
      "train Loss: 1.0588 Acc: 0.8298\n",
      "val Loss: 0.6970 Acc: 0.8571\n",
      "\n",
      "Epoch 73/99\n",
      "-------------------------\n",
      "train Loss: 1.7271 Acc: 0.7234\n",
      "val Loss: 0.6996 Acc: 0.8776\n",
      "\n",
      "Epoch 74/99\n",
      "-------------------------\n",
      "train Loss: 1.3922 Acc: 0.6809\n",
      "val Loss: 0.7198 Acc: 0.8980\n",
      "\n",
      "Epoch 75/99\n",
      "-------------------------\n",
      "train Loss: 1.4612 Acc: 0.7021\n",
      "val Loss: 0.7078 Acc: 0.8776\n",
      "\n",
      "Epoch 76/99\n",
      "-------------------------\n",
      "train Loss: 1.4009 Acc: 0.7234\n",
      "val Loss: 0.7263 Acc: 0.8571\n",
      "\n",
      "Epoch 77/99\n",
      "-------------------------\n",
      "train Loss: 1.6770 Acc: 0.7447\n",
      "val Loss: 0.7126 Acc: 0.8571\n",
      "\n",
      "Epoch 78/99\n",
      "-------------------------\n",
      "train Loss: 1.2649 Acc: 0.7872\n",
      "val Loss: 0.7233 Acc: 0.8571\n",
      "\n",
      "Epoch 79/99\n",
      "-------------------------\n",
      "train Loss: 1.5655 Acc: 0.7021\n",
      "val Loss: 0.7205 Acc: 0.8571\n",
      "\n",
      "Epoch 80/99\n",
      "-------------------------\n",
      "train Loss: 1.3606 Acc: 0.7660\n",
      "val Loss: 0.7164 Acc: 0.8776\n",
      "\n",
      "Epoch 81/99\n",
      "-------------------------\n",
      "train Loss: 1.4827 Acc: 0.7021\n",
      "val Loss: 0.7487 Acc: 0.8776\n",
      "\n",
      "Epoch 82/99\n",
      "-------------------------\n",
      "train Loss: 1.6195 Acc: 0.7234\n",
      "val Loss: 0.7120 Acc: 0.8571\n",
      "\n",
      "Epoch 83/99\n",
      "-------------------------\n",
      "train Loss: 1.1506 Acc: 0.8085\n",
      "val Loss: 0.7213 Acc: 0.8571\n",
      "\n",
      "Epoch 84/99\n",
      "-------------------------\n",
      "train Loss: 1.6827 Acc: 0.7021\n",
      "val Loss: 0.7057 Acc: 0.8571\n",
      "\n",
      "Epoch 85/99\n",
      "-------------------------\n",
      "train Loss: 1.7738 Acc: 0.7021\n",
      "val Loss: 0.7122 Acc: 0.8571\n",
      "\n",
      "Epoch 86/99\n",
      "-------------------------\n",
      "train Loss: 1.0528 Acc: 0.8298\n",
      "val Loss: 0.7420 Acc: 0.8571\n",
      "\n",
      "Epoch 87/99\n",
      "-------------------------\n",
      "train Loss: 1.2746 Acc: 0.8298\n",
      "val Loss: 0.7384 Acc: 0.8776\n",
      "\n",
      "Epoch 88/99\n",
      "-------------------------\n",
      "train Loss: 1.6197 Acc: 0.7234\n",
      "val Loss: 0.7325 Acc: 0.8571\n",
      "\n",
      "Epoch 89/99\n",
      "-------------------------\n",
      "train Loss: 1.3692 Acc: 0.7234\n",
      "val Loss: 0.7253 Acc: 0.8571\n",
      "\n",
      "Epoch 90/99\n",
      "-------------------------\n",
      "train Loss: 1.4475 Acc: 0.7447\n",
      "val Loss: 0.7142 Acc: 0.8367\n",
      "\n",
      "Epoch 91/99\n",
      "-------------------------\n",
      "train Loss: 1.4299 Acc: 0.7447\n",
      "val Loss: 0.7119 Acc: 0.8571\n",
      "\n",
      "Epoch 92/99\n",
      "-------------------------\n",
      "train Loss: 1.5623 Acc: 0.7447\n",
      "val Loss: 0.6998 Acc: 0.8367\n",
      "\n",
      "Epoch 93/99\n",
      "-------------------------\n",
      "train Loss: 1.5359 Acc: 0.7021\n",
      "val Loss: 0.6882 Acc: 0.8367\n",
      "\n",
      "Epoch 94/99\n",
      "-------------------------\n",
      "train Loss: 1.5715 Acc: 0.7872\n",
      "val Loss: 0.6989 Acc: 0.8367\n",
      "\n",
      "Epoch 95/99\n",
      "-------------------------\n",
      "train Loss: 1.8671 Acc: 0.6383\n",
      "val Loss: 0.7076 Acc: 0.8571\n",
      "\n",
      "Epoch 96/99\n",
      "-------------------------\n",
      "train Loss: 1.4377 Acc: 0.7660\n",
      "val Loss: 0.6737 Acc: 0.8571\n",
      "\n",
      "Epoch 97/99\n",
      "-------------------------\n",
      "train Loss: 1.4338 Acc: 0.7447\n",
      "val Loss: 0.6632 Acc: 0.8571\n",
      "\n",
      "Epoch 98/99\n",
      "-------------------------\n",
      "train Loss: 1.3709 Acc: 0.7872\n",
      "val Loss: 0.6724 Acc: 0.8571\n",
      "\n",
      "Epoch 99/99\n",
      "-------------------------\n",
      "train Loss: 1.6653 Acc: 0.7234\n",
      "val Loss: 0.6662 Acc: 0.8776\n",
      "\n",
      "Training complete in 6m 50s\n",
      "Best val Acc: 0.8980\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer_ft, scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f92564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model weights saved to finetuned_resnet50_full_1000class100epochs.pth\n"
     ]
    }
   ],
   "source": [
    "save_path = 'finetuned_resnet50_full_1000class100epochs.pth'\n",
    "torch.save(model_ft.state_dict(), save_path)\n",
    "print(f\"\\nBest model weights saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d8ad3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ce88083",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()\n",
    "categories = weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e421d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_name):\n",
    "    if not os.path.exists(image_name):\n",
    "        print(f\"ERROR: File not found at path: {image_name}\")\n",
    "        return None\n",
    "        \n",
    "    image = Image.open(image_name).convert(\"RGB\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0503d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "glare_images = './glare_distortion/glare_images/'\n",
    "results = {}\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "141070c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "252f9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(filename):\n",
    "    match = re.search(r'glare\\d+(.*?)\\.jpg$', filename)\n",
    "    if match:\n",
    "        return match.group(1) \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fdaaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(input_batch, image_type, file_key, verbose=False):\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "        \n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    \n",
    "    # Get Top 5 Predictions\n",
    "    top5_prob, top5_indices = torch.topk(probabilities, 5)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        predicted_index = top5_indices[i].item()\n",
    "        predicted_label = categories[predicted_index]\n",
    "        probability = top5_prob[i].item()\n",
    "        predictions.append({\n",
    "            \"rank\": i + 1,\n",
    "            \"label\": predicted_label,\n",
    "            \"probability\": probability\n",
    "        })\n",
    "    \n",
    "    # Get the single best prediction \n",
    "    best_pred = predictions[0]\n",
    "    \n",
    "    results[file_key][image_type] = {\n",
    "        \"best_label\": best_pred[\"label\"],\n",
    "        \"best_probability\": best_pred[\"probability\"],\n",
    "        \"top_5_predictions\": predictions\n",
    "    }\n",
    "    if(verbose):     \n",
    "        print(f\"  {image_type.capitalize()} Best Label: **{best_pred['label']}** (P: {best_pred['probability']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b32ba6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cv_image(cv_img, preprocess_func):\n",
    "    if cv_img.ndim == 2:\n",
    "        rgb_img = np.stack([cv_img, cv_img, cv_img], axis=2)\n",
    "    else:\n",
    "        rgb_img = cv_img\n",
    "\n",
    "    pil_image = Image.fromarray(rgb_img)\n",
    "    \n",
    "    input_tensor = preprocess_func(pil_image)\n",
    "    \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dde47d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_label_match(ground_truth, predicted_label):\n",
    "    gt = ground_truth.lower().replace(\" \", \"\")\n",
    "    pl = predicted_label.lower().replace(\" \", \"\")\n",
    "    # print(f\"ground truth: {gt} == {pl} : predicted\")\n",
    "    if gt in pl:\n",
    "        return True\n",
    "    \n",
    "    if pl in gt:\n",
    "        return True\n",
    "        \n",
    "    if gt == pl:\n",
    "        return True\n",
    "        \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e93197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch prediction on device: cuda:0\n",
      "--------------------------------------------------\n",
      "Batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "print(f\"Starting batch prediction on device: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for filename in os.listdir(glare_images):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        \n",
    "        original_file_path = os.path.join(glare_images, filename)\n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"Processing: **{filename}**\")\n",
    "        \n",
    "        file_key = os.path.splitext(filename)[0]\n",
    "        results[file_key] = {\n",
    "            \"original\": {},\n",
    "            \"processed\": {},\n",
    "            \"label\" : getLabel(filename)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            image_original = image_loader(original_file_path)\n",
    "            \n",
    "            input_tensor_original = preprocess(image_original)\n",
    "            \n",
    "            input_batch_original = input_tensor_original.unsqueeze(0).to(device)\n",
    "            \n",
    "            get_predictions(input_batch_original, \"original\", file_key)\n",
    "            if(verbose):\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing **{filename}**: {e}\")\n",
    "            del results[file_key] # Remove incomplete result\n",
    "            print(\"-\" * 50)\n",
    "            continue\n",
    "            \n",
    "print(\"Batch processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b9e69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch prediction on device: cuda:0\n",
      "--------------------------------------------------\n",
      "Batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "# For Finetuned Version\n",
    "verbose = False\n",
    "print(f\"Starting batch prediction on device: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for filename in os.listdir(glare_images):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        \n",
    "        original_file_path = os.path.join(glare_images, filename)\n",
    "\n",
    "        if(verbose):\n",
    "            print(f\"Processing: **{filename}**\")\n",
    "        \n",
    "        file_key = os.path.splitext(filename)[0]\n",
    "        results[file_key] = {\n",
    "            \"original\": {},\n",
    "            \"processed\": {},\n",
    "            \"label\" : getLabel(filename)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            image_original = image_loader(original_file_path)\n",
    "            \n",
    "            input_tensor_original = preprocess(image_original)\n",
    "            \n",
    "            input_batch_original = input_tensor_original.unsqueeze(0).to(device)\n",
    "            \n",
    "            get_predictions(input_batch_original, \"original\", file_key)\n",
    "            if(verbose):\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing **{filename}**: {e}\")\n",
    "            del results[file_key] # Remove incomplete result\n",
    "            print(\"-\" * 50)\n",
    "            continue\n",
    "            \n",
    "print(\"Batch processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c13f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_side_by_side_comparison(results_dict, print_results=False):\n",
    "    max_label_width = 0\n",
    "    \n",
    "    # Find the longest label across all Top 5 lists\n",
    "    for data in results_dict.values():\n",
    "        for pred in data[\"original\"][\"top_5_predictions\"]:\n",
    "            max_label_width = max(max_label_width, len(pred[\"label\"]))\n",
    "\n",
    "    label_padding = max_label_width + 12\n",
    "    orig_top1 = 0\n",
    "    original_correct = 0\n",
    "    for file_key, data in results_dict.items():\n",
    "        original_preds = data[\"original\"][\"top_5_predictions\"]\n",
    "        ground_truth = data['label'] \n",
    "        if(print_results):\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"**{file_key}** (Ground Truth: {ground_truth})\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            header_format = f\"{'Original Top 5':<{label_padding}} | {'Processed Top 5'}\"\n",
    "            print(header_format)\n",
    "            print(\"-\" * 70)\n",
    "\n",
    "        original_is_counted = False\n",
    "\n",
    "        for i in range(5):\n",
    "            orig_label = original_preds[i]['label']\n",
    "            orig_prob = original_preds[i]['probability']\n",
    "            orig_is_match = check_label_match(ground_truth, orig_label)\n",
    "            if i == 0 and orig_is_match:\n",
    "                orig_top1 += 1\n",
    "            orig_star = \"*\" if orig_is_match else \"\"\n",
    "            if orig_is_match and not original_is_counted:\n",
    "                original_correct += 1 \n",
    "                original_is_counted = True\n",
    "                original_prob = orig_prob\n",
    "            \n",
    "            orig_output = f\"{orig_label}: {orig_prob:.4f}{orig_star}\"\n",
    "            \n",
    "            # Use f-string formatting to align the original column based on max width\n",
    "            comparison_line = f\"{orig_output:<{label_padding}}\" \n",
    "            \n",
    "            if(print_results):\n",
    "                print(comparison_line)\n",
    "\n",
    "        \n",
    "    if(print_results):\n",
    "        print(\"=\" * 70)\n",
    "    return original_correct, orig_top1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c70f01",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "198f1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results = False\n",
    "original_correct, orig_top1 = print_side_by_side_comparison(results, print_results=print_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5adba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 31]\n"
     ]
    }
   ],
   "source": [
    "print([original_correct,orig_top1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05688c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Acc: 0.6229508196721312\n"
     ]
    }
   ],
   "source": [
    "print(f'OG Acc: {original_correct/(len(os.listdir(glare_images)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce566a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Acc Best Guess: 0.5081967213114754\n"
     ]
    }
   ],
   "source": [
    "print(f'OG Acc Best Guess: {orig_top1/(len(os.listdir(glare_images)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4449b",
   "metadata": {},
   "source": [
    "# Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "060a3301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results = False\n",
    "original_correct, orig_top1 = print_side_by_side_comparison(results, print_results=print_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b67998af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 26]\n"
     ]
    }
   ],
   "source": [
    "print([original_correct,orig_top1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f8cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Acc: 0.5245901639344263\n",
      "OG Acc Best Guess: 0.4262295081967213\n"
     ]
    }
   ],
   "source": [
    "print(f'OG Acc: {original_correct/(len(os.listdir(glare_images)))}')\n",
    "print(f'OG Acc Best Guess: {orig_top1/(len(os.listdir(glare_images)))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
